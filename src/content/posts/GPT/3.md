---
title: 复刻GPT2对话模型(讲解篇1)
published: 2025-01-27
description: '从入门细节讲解 GPT2对话模型'
image: ''
tags: [人工智能,大模型]
category: '人工智能'
draft: false 
lang: ''
---
# 前言
在看了前两期入门篇之后，相信各位一定入门失败了，那么作为枫叶来说如果不能讲清楚那么=白讲，所有我们开一个新赛道来讲，主要围绕

**Transformer架构**->**pytorch框架**->**TensorFlow**->**自然语言处理(NLP)**

当然还是分多期来讲解，有人肯定会问为什么不录制视频进行讲解，很明显对于我来说还是太有挑战性了，首先得一边录制一边讲解，还得不出差错，表达需要清晰，还需要进行剪辑，带了背景音乐提神会显得很吵，不带又无聊，视频上传后还得考虑服务器带宽，上架短视频又会有一堆人进行评论“讲解”，带偏节奏，在加上我们四川人喜欢讲方言会导致大部分人听不懂，而文字作为统一的，不会纯在各地不认识的情况

# 说明
还是在开始教学之前说明，我不是专业的(不是专业的，不是专业的，不是专业的)懂？

本人是兴趣爱好者，单纯的分享自己学到的，并把那些很高级很难懂的词语或者句子，按照大白话进行拆分讲解，因为我自己也算是爱好者吧，当初自己在入门时遇到很多难点，比如资料或者博客差缺，我一种不能理解的事情就是，你写的博客不就是分享技术的吗？没必要付费阅读吧，我对于那些付费才能阅读剩下的文章感到反感，对于国内某些up主，搬运别人的代码素材，从来不指出原文作者，更有高手把别人代码收费，明明在原视频中这些代码都是公开透明的，单纯无法理解这样的行为我表示反感

如果我讲解的有问题还请各位指出问题我会及时进行修改，以方便更多兴趣爱好者学习交流

本期我们讲解**Transformer架构**

长文警告

# Transformer架构
Transformer架构是由Google在2017年提出来的，它使用了一种新的架构， 叫 **self-attention**，这个架构可以让模型学习到长距离的依赖关系，而不需要使用循环神经网络来处理序列数据，全新的简单网络架构，完全基于注意力机制[**Transformer架构论文**](https://arxiv.org/pdf/1706.03762)

我还是简单讲解，从讲解论文开始吧，(接下来讲解的论文由OpenAI进行翻译)

摘要(由OpenAI进行翻译-枫叶纠正)

现有主流的序列转换模型主要基于复杂的循环神经网络（RNN）或卷积神经网络（CNN），并采用编码器-解码器架构。表现最好的模型通常通过注意力机制将编码器和解码器连接起来。我们提出了一种全新的简单网络架构——Transformer，仅基于注意力机制，完全摒弃了循环和卷积结构。在两个机器翻译任务上的实验表明，这些模型在质量上表现更优，同时具备更强的并行性，并显著降低了训练所需时间。在WMT 2014英德翻译任务中，我们的模型达到了28.4的BLEU分数，比现有最佳结果（包括集成模型）提升了超过2个BLEU。在WMT 2014英法翻译任务中，我们的模型在8块GPU上仅训练了3.5天，就取得了41.8的单模型最新最佳BLEU分数，大幅降低了文献中其他最佳模型的训练成本。此外，我们通过在大数据量和有限数据量的条件下成功应用Transformer于英语句法解析任务，证明了其在其他任务中的良好泛化能力

解释(由OpenAI进行讲解-枫叶纠正)
## 1. **RNN（Recurrent Neural Network）**
**全称**：循环神经网络  
**主要功能**：  
RNN 是一种神经网络架构，专为处理序列数据（如时间序列、文本、音频等）设计。它通过循环结构保留上下文信息，使网络能够理解输入数据的时间或顺序关系。  

**工作原理**：  
RNN 的关键是隐藏层的状态会随着时间步递归更新，将前一时刻的信息传递到下一时刻。  
公式：  

$$
h_t = f(W \cdot x_t + U \cdot h_{t-1} + b)
$$

其中：  
- \( h_t \)：当前时刻的隐藏状态  
- \( x_t \)：当前输入  
- \( h_{t-1} \)：上一时刻的隐藏状态  

**优点**：  
- 擅长处理顺序相关的数据，如文本生成、语音识别。  

**缺点**：  
- 存在梯度消失或梯度爆炸问题，训练长序列时效果下降。  
- 训练时间长，不易并行化。  

---

## 2. **CNN（Convolutional Neural Network）**
**全称**：卷积神经网络  
**主要功能**：  
CNN 是一种神经网络，最初用于图像处理，但也可扩展到处理一维（如文本）或多维数据。其核心特点是通过卷积操作提取局部特征，尤其擅长捕捉空间或局部相关性。

**工作原理**：  
- 卷积层：通过卷积核扫描输入数据，提取局部特征。  
- 池化层：降低数据维度，减少计算量，同时保留主要特征。  
- 全连接层：将提取到的特征进行整合，输出最终预测结果。  

**优点**：  
- 参数较少，计算效率高。  
- 能捕获局部模式，适合处理图像和语音信号。  

**缺点**：  
- 需要大量数据才能有效训练。  
- 对长距离依赖关系的建模能力不足。  

---

## 3. **WMT（Workshop on Machine Translation）**
**全称**：机器翻译研讨会  
**主要功能**：  
WMT 是机器翻译领域的重要年度活动，包括竞赛和研讨会。它提供标准化的数据集和评价指标，推动机器翻译模型的发展。  

**内容包括**：  
- 数据集：WMT 提供多语言对的翻译数据集，如英语-德语、英语-法语等。  
- 竞赛：研究者提交模型参与翻译任务评比，比较性能。  
- 评价：基于 BLEU 等指标评估模型质量。  

WMT 数据集的规模和质量，使其成为机器翻译领域的基准。  

---

## 4. **BLEU（Bilingual Evaluation Understudy）**
**全称**：双语评估替代指标  
**主要功能**：  
BLEU 是机器翻译的评价指标，用来衡量机器生成的翻译文本与人工参考翻译之间的相似度。得分范围为 0 到 1（通常以百分比表示），得分越高表示翻译质量越好。  

**计算原理**：  
1. **n-gram 匹配**：统计机器翻译文本和参考翻译中 n-gram（连续 n 个单词短语）的重合程度。  
2. **惩罚机制**：防止模型生成过短的句子，通过长度惩罚修正得分。  

公式：  

$$
BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
$$

- \( BP \)：长度惩罚因子  
- \( p_n \)：n-gram 的匹配精度  
- \( w_n \)：n-gram 的权重  

**优点**：  
- 自动化且快速计算。  
- 适合评估多语言翻译任务。  

**缺点**：  
- 过于注重表面词汇匹配，可能忽略翻译的语义质量。  

当然对与GPT的翻译加解释我相信各位还是看不懂，那么由我来进行通俗易懂的大白话讲解

# Transformer简单理解

Transformer主要的是**RNN**->**CNN**

## RNN
RNN是循环神经网络全称（Recurrent Neural Network）多结构样式

<img src="https://1drv.ms/i/c/2182f48b953d36f8/IQRMlPYpdPcWRrc9K4D8nJVLAQ2efKD5Lycvt6ZACr98IAU?width=1024" width="1024" height="auto" />

详细可以到[RNN讲解](https://blog.csdn.net/bestrivern/article/details/90723524)

## CNN
CNN是卷积神经网络全称（Convolutional Neural Network）主要由，输入层->卷积层

输入层表示 CNN 的输入图像,因为我们使用 RGB 图像作为输入，所以输入层有三个通道，分别对应红色、绿色和蓝色通道，显示在该层中

卷积层是 CNN 的基础，因为它们包含学习的内核（权重），这些内核提取区分不同图像的特征——这就是我们想要的分类！当您与卷积层交互时，您会注意到前几层和卷积层之间的链接。每个链接代表一个唯一的内核，用于卷积操作以生成当前卷积神经元的输出或激活图
卷积神经元使用唯一的内核和上一层相应神经元的输出执行元素点积。这将产生与唯一内核一样多的中间结果。卷积神经元是所有中间结果与学习偏差相加的结果。

详细可以到[CNN讲解](https://poloclub.github.io/cnn-explainer/)国内可以到[csdn的CNN讲解](https://blog.csdn.net/AI_dataloads/article/details/133250229)

# 注意力机制
注意力机制是Transformer的核心，它通过计算两个向量之间的相似度来确定它们之间的权重，从而实现对输入的关注。就好比人在看到一张图片时，会根据图片中包含的内容，选择关注哪些区域，然后根据这些关注区域的权重来确定最终的输出。
例如看到一个好看的景色，入会首先注意到主要元素，然后在搭配其他进行欣赏

<img src="https://1drv.ms/i/c/2182f48b953d36f8/IQRersCnJbxiQpy--A4d3_jzAa0pO5hcI8utI9z5wP0T6R8?width=1024" width="1024" height="auto" />

公式为
<img src="https://1drv.ms/i/c/2182f48b953d36f8/IQSYpIgpRlUOQ6MK2rTzk9rQAQkhi3XqU1a8D7uIiM3CMxs?width=1024" width="1024" height="auto" />

图像和公式来源谷歌的论文[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

对于这个公式和数据，看看就好毕竟我们也不算专家，专家请点击对应论文跳转到相应的公式和论文进行学习，你可以把我的博客当成跳板方便查找对应论文

国内可以阅读[Self-Attention](https://blog.csdn.net/weixin_42110638/article/details/134016569)

本期应该就没什么讲解的了如有遗漏请补充留言